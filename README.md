# Multiâ€‘Modal Deep Learning for Joint Prediction of Drugâ€“Target Interaction & Sideâ€‘Effects

> **Oneâ€‘pager:** Multiâ€‘modal, multiâ€‘task model that jointly predicts **binding affinity (pKI)** and **sideâ€‘effects** from SMILES (sequence + 2D image) and **protein embeddings**.

<p align="center">
  <img src="assets/drug target interaction.png" alt="Drugâ€“Target Interaction architecture" width="85%"/>
</p>

<p align="center">
  <img src="assets/side effect prediction.png" alt="Sideâ€‘Effect Prediction architecture" width="85%"/>
</p>

---

## âœ¨ Highlights

* **Multiâ€‘modal inputs**: SMILES **sequence** â†’ Transformer, SMILES **image** â†’ CNN; **protein** sequences â†’ ProtBERT + linear head.
* **Multiâ€‘task learning**:

  * **DTI**: regression on **pKI**
  * **Sideâ€‘effects**: multiâ€‘label classification
* **Fusion**: image + text â†’ drug embedding â†’ fused with protein embedding.
* **Datasets**: **BinderDB** (affinity) and **SIDER** (sideâ€‘effects), protein sequences via **UniProt**.

## ðŸ“¦ Repository Structure

```
src/
 â”œâ”€ dataset.py        # Data loading & preprocessing (SMILES, images, proteins)
 â”œâ”€ model.py          # CNN, Transformer, ProtBERT head, fusion, multitask heads
 â”œâ”€ train.py          # Training loop, multitask losses (MSE + BCE), metrics
 â”œâ”€ evaluate.py       # Eval for DTI (RMSE/MSE) & SE (mAP/F1/partial acc)
 â”œâ”€ utils.py          # Helpers (seeding, logging, checkpointing)
 â”œâ”€ sdf_reader.py     # SDF/SMILES utilities (RDKit)
 â””â”€ config.json       # Hyperparams & paths
```

## ðŸ”§ Setup

```bash
# Python 3.9+
python -m venv .venv && source .venv/bin/activate  # (Windows: .venv\Scripts\activate)
pip install -r requirements.txt
```

**Core deps:** `torch`, `transformers`, `rdkit-pypi`, `pandas`, `numpy`, `scikit-learn`, `opencv-python`

Create a `.env` or edit `config.json` to point to your data folders.

## ðŸ—‚ï¸ Data

* **BinderDB** â†’ drugâ€“target affinity (Ki â†’ **pKI = âˆ’log10(Ki)**)
* **SIDER** â†’ sideâ€‘effects for \~1.4k drugs mapped to 5,880 terms
* **UniProt/ProtBERT** â†’ protein sequence embeddings (frozen or fineâ€‘tuned)

Convert SMILES to 2D images with **RDKit**; tokenized SMILES feed the Transformer.

## â–¶ï¸ Train

```bash
python src/train.py --config src/config.json
```

Key config knobs: batch size, learning rate, loss weights, image size (e.g., 64Ã—64), transformer dims, ProtBERT freeze.

## ðŸ“ˆ Evaluate

```bash
python src/evaluate.py --checkpoint runs/best.ckpt --split test
```

**DTI**: RMSE/MSE on pKI.
**SE**: mAP/F1/partialâ€‘accuracy.

## ðŸ§ª Results (course project)

* **SE head** benefited from multitask learning; **DTI** regressor mildly regressed due to gradient conflict.
* Next: apply **PCGrad/GradNorm** for conflict mitigation; explore **pKd/IC50**.

## ðŸ—ºï¸ Roadmap

* [ ] Gradient surgery (PCGrad) for multiâ€‘task stability
* [ ] Fineâ€‘tune ProtBERT endâ€‘toâ€‘end
* [ ] Add pKd/IC50 objectives & uncertainty calibration
* [ ] Weights & Biases logging + model cards

## ðŸ“ Citation

> Yu et al., **Gradient Surgery for Multiâ€‘Task Learning**, arXiv:2001.06782.

## ðŸ‘¥ Authors

* **Nishanth G. Palaniswami (NG3124)**
* **Arun Purohit (AP9111)**

## ðŸ“„ License

MIT
